# AI Context and Guidance for cvideo-click-pave Repository

This file provides comprehensive guidance for AI assistants working on this repository. Read this file first before making any changes to understand the project's architecture, patterns, and constraints.

## Project Overview

**Repository Purpose**: AWS infrastructure provisioning for the cvideo-click project using Infrastructure as Code (IaC) principles.

**Core Technologies**:

- **Terraform**: Primary infrastructure management tool
- **Python + boto3**: All automation scripts and AWS interactions
- **Make**: Workflow orchestration and task automation
- **GitHub Actions**: CI/CD pipeline with intelligent local/production detection

**No Bash/Shell Scripts**: All automation must use Python and boto3. Bash scripts are deprecated.

## Code Quality Standards

### **Python Development Standards**

- **Formatter**: Black with 88-character line length
- **Linting**: Flake8 with E203/W503 exceptions for Black compatibility
- **Type Checking**: mypy with relaxed settings for boto3 compatibility
- **Documentation**: PEP257 compliant docstrings for all functions
- **Logging**: Structured logging with timestamps and proper levels
- **Error Handling**: Comprehensive exception handling with boto3 ClientError patterns

### **Configuration Files**

- `pyproject.toml`: Black and mypy configurations
- `requirements.txt`: Production and development dependencies including boto3-stubs
- `Makefile`: Code quality targets (format, lint, type-check, validate)

### **Development Workflow**

```bash
make format      # Format code with Black
make lint        # Lint with Flake8
make type-check  # Type checking with mypy
make security    # Security scan for secrets and vulnerabilities
make validate    # Comprehensive validation (includes security + formatting + linting + terraform)
make state-show  # Display current Terraform state resources
make state-pull  # Pull remote state for local inspection
make state-backup # Create timestamped backup of current state
```

### **ðŸ”’ MANDATORY Security Requirements**

**CRITICAL**: ALWAYS run security checks before committing any changes:

1. **Pre-Commit Security Scan**: Run `make security` before every git commit
1. **Secret Detection**: Automatically scans for exposed AWS keys, passwords, tokens
1. **File Permissions**: Verifies .secrets file has secure 600 permissions
1. **Git Exclusions**: Ensures sensitive files are properly excluded from version control
1. **Zero Tolerance**: No commits allowed with exposed secrets or security issues

**Security Command Details**:

- Scans all code files for secret patterns (excluding comments)
- Detects hardcoded AWS access keys (AKIA pattern)
- Validates .secrets file permissions (must be 600)
- Checks .gitignore for sensitive file exclusions
- Reports all findings with line numbers and file locations

### **Terraform State Management**

- **Remote S3 Backend**: `pave-tf-state-bucket-us-east-1` with key `pave/terraform.tfstate`
- **Shared State**: All deployment methods (local, Act, GitHub Actions) use the same S3 backend
- **Migration**: Completed migration from local backend to S3 remote backend (23 resources)
- **State Operations**: Makefile includes state management targets for backup and inspection

## Architecture Principles

### 1. **Unified Workflow Philosophy**

- Single GitHub Actions workflow (`terraform.yaml`) that intelligently adapts to environments
- Environment detection via `${{ env.ACT }}` for local (Act) vs production (GitHub Actions)
- Consistent behavior across local development and CI/CD
- **Shared Terraform State**: S3 remote backend ensures state consistency across all deployment methods

### 2. **Technology Stack Constraints**

- âœ… **Terraform**: Infrastructure definition and state management (S3 remote backend)
- âœ… **Python + boto3**: All AWS interactions and automation scripts
- âœ… **Make**: Task orchestration (replaces complex shell commands)
- âœ… **GitHub Actions**: CI/CD pipeline
- âœ… **Black Formatter**: Consistent Python code formatting
- âœ… **Type Annotations**: Full typing support with mypy
- âŒ **No Bash/Shell scripts**: Use Python instead
- âŒ **No AWS CLI calls in scripts**: Use boto3 programmatically

### 3. **Authentication Strategy**

- **Local Development**: AWS access keys via `.secrets` file
- **GitHub Actions**: AWS access keys via repository secrets (simplified approach)
- **No OIDC**: Avoided due to chicken-and-egg problem (this repo creates the OIDC role)

### 4. **Resource Naming Convention**

All AWS resources use consistent, predictable naming:

```text
admin-user
developer-user  
CICDDeploymentRole
pave-tf-state-bucket-us-east-1
```

### 5. **Security Model - Bootstrap User Pattern**

**CRITICAL**: This repository implements a bootstrap user security model:

- **Bootstrap User**: `bootstrap-user` - Created manually by AWS root account
- **Bootstrap Role**: `PaveBootstrapRole` - Administrative role for infrastructure management
- **Bootstrap Policy**: `PaveBootstrapPolicy` - Full permissions except cannot delete bootstrap resources
- **Purpose**: Provides secure, permanent administrative access for infrastructure management
- **Protection**: NEVER managed by Terraform, NEVER deleted by cleanup scripts

#### **Managed Tier (Terraform Created)**

- **Admin User**: Limited administrative access, cannot delete bootstrap resources
- **Developer User**: Limited access (S3 + Lambda + EC2 read-only) for application development
- **CI/CD Roles**: Specific permissions for deployment automation
- **Protection**: Managed by Terraform, can be cleaned up safely

#### **Authentication Flow**

1. **Repository Authentication**: Uses bootstrap user credentials in secrets
1. **All Operations**: Run as bootstrap user with full administrative privileges
1. **Created Resources**: Cannot modify or delete bootstrap user/role/policy

#### **Security Constraints**

- **Admin users cannot delete bootstrap resources**: Explicit deny policies prevent this
- **Credential Files**: Template-based approach due to AWS access key secrets being non-retrievable
- **File Permissions**: 600 for all credential files
- **Bootstrap Resources**: Protected from all automation and cleanup operations
- **Git Ignore**: All credentials and sensitive files excluded

## File Structure and Responsibilities

```text
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ terraform.yaml         # Unified intelligent workflow
â”œâ”€â”€ scripts/                   # Python automation scripts (PEP 8 compliant)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ credentials.py         # Credential management with full typing and logging
â”‚   â””â”€â”€ cleanup.py             # AWS resource cleanup with comprehensive error handling
â”œâ”€â”€ pave_infra.tf              # Main Terraform configuration
â”œâ”€â”€ Makefile                   # Task orchestration with code quality targets
â”œâ”€â”€ requirements.txt           # Production and development dependencies
â”œâ”€â”€ pyproject.toml             # Black and mypy configuration
â”œâ”€â”€ .ai-context.md             # This file - AI guidance
â”œâ”€â”€ .secrets                   # Local AWS credentials (gitignored)
â”œâ”€â”€ .actrc                     # Act configuration
â”œâ”€â”€ credentials/               # Generated credential templates (gitignored)
â”œâ”€â”€ GITHUB_SETUP.md            # Repository secrets setup guide
â””â”€â”€ README.md                  # User documentation
```

## Makefile Interface Design

The Makefile provides these primary targets with integrated code quality:

```makefile
# Core Infrastructure Operations
make init          # Initialize terraform and install Python deps
make plan          # Terraform plan
make apply         # Terraform apply (deploy infrastructure)  
make destroy       # Terraform destroy
make clean         # Comprehensive cleanup of all AWS resources

# Credential Management
make credentials   # Extract/generate credential templates
make setup-github  # Set up GitHub repository secrets

# Development Workflow
make dev-deploy    # Local development deployment (clean slate)
make dev-clean     # Clean up development resources

# Code Quality
make format        # Format code with Black
make lint          # Lint code with Flake8
make type-check    # Type check with mypy
make validate      # Validate terraform + Python code (includes formatting/linting)
make test          # Run tests if any

# Help
make help          # Show all available targets
```

## Python Script Guidelines

### 1. **Code Quality Standards**

- **Black Formatting**: 88-character line length, PEP8 compliant
- **Type Annotations**: Full typing with boto3-stubs support
- **Logging**: Structured logging with proper levels and timestamps
- **Error Handling**: Comprehensive boto3 ClientError handling
- **Documentation**: PEP257 compliant docstrings

### 2. **scripts/credentials.py**

- Professional-grade credential management with full type annotations
- Comprehensive error handling and structured logging
- Proper file permissions and security practices
- Clear user guidance for manual credential extraction

### 3. **scripts/cleanup.py**

- Comprehensive AWS resource cleanup with bootstrap protection
- Type-safe boto3 operations with proper exception handling
- Structured logging with progress reporting
- Resource validation and rollback capabilities

### 4. **Common Patterns for Python Scripts**

```python
"""Module docstring following PEP257."""
import logging
from typing import Optional, Dict, Any
import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_boto3_client(service: str) -> boto3.client:
    """Get boto3 client with proper error handling and type hints."""
    try:
        return boto3.client(service)
    except NoCredentialsError:
        logger.error(f"No AWS credentials found for {service}")
        raise
    except ClientError as e:
        logger.error(f"Error connecting to AWS {service}: {e}")
        raise

def process_resources(client: boto3.client, pattern: str) -> Optional[Dict[str, Any]]:
    """Process AWS resources with comprehensive error handling."""
    try:
        # Implementation with proper type hints and logging
        logger.info(f"Processing resources with pattern: {pattern}")
        # ... implementation
    except ClientError as e:
        logger.error(f"AWS API error: {e}")
        return None
```

## Workflow Integration Points

### 1. **Local Development with Act**

- Use `make dev-deploy` which calls terraform via Act
- Automatically runs cleanup first for clean slate testing
- Uses `.secrets` file for authentication

### 2. **GitHub Actions**

- Triggered by push to main or workflow_dispatch
- Uses repository secrets for authentication
- Calls `make plan` and `make apply`

### 3. **Direct Terraform**

- `make init`, `make plan`, `make apply` for traditional workflow
- Python scripts handle pre/post terraform operations

## Error Handling Patterns

### 1. **Graceful Degradation**

- If terraform outputs not available, fall back to AWS API queries
- If resources don't exist, continue without errors (idempotent operations)
- Clear error messages with actionable suggestions

### 2. **Validation**

- Validate AWS credentials before operations
- Check for required tools (terraform, python, boto3)
- Validate terraform configuration before apply

### 3. **Rollback Capabilities**

- Cleanup operations should be idempotent
- Provide dry-run modes for destructive operations
- State preservation during partial failures

## Testing Strategy

### 1. **Local Testing**

- Use Act for GitHub Actions workflow testing
- `make dev-deploy` for full infrastructure testing
- `make clean` for cleanup validation

### 2. **Validation Commands**

- `make validate` should check terraform syntax and Python code
- AWS connectivity tests
- Required dependency checks

## Documentation Standards

### 1. **README.md**

- Document Makefile interface as primary user interface
- Include quick start with Makefile commands
- Troubleshooting section with make targets

### 2. **Code Comments**

- Python scripts should have docstrings for all functions
- Makefile targets should have comment descriptions
- Terraform resources should have meaningful descriptions

## Migration from Shell Scripts

### Deprecated Files to Remove

- `extract-credentials.sh` â†’ Replace with `scripts/credentials.py`
- `get-credentials.sh` â†’ Replace with `scripts/credentials.py`
- `cleanup-all.sh` â†’ Replace with `scripts/cleanup.py`

### Functionality Preservation

Ensure all functionality from shell scripts is preserved in Python equivalents:

1. **Credential extraction** with terraform output fallback to boto3
1. **Template file generation** with AWS Console instructions
1. **Comprehensive cleanup** of all pave resources across deployments
1. **Progress reporting** with clear status messages
1. **Error handling** for missing resources or permissions

## AI Assistant Guidelines

### When Working on This Repository

1. **Read this file first** to understand architecture and constraints
1. **Use Python + boto3** instead of AWS CLI or shell scripts
1. **Maintain Makefile interface** as primary user interaction
1. **Preserve security model** with proper credential handling
1. **Test with Act** before suggesting GitHub Actions changes
1. **Update documentation** when changing interfaces

### Common Tasks

- **Adding new infrastructure**: Update `pave_infra.tf` and relevant Makefile targets
- **AWS interactions**: Always use boto3 in Python scripts, not AWS CLI
- **Workflow changes**: Test locally with Act first
- **Credential management**: Use template-based approach due to AWS limitations

### Anti-Patterns to Avoid

- âŒ Using bash/shell scripts for new functionality
- âŒ AWS CLI calls in automation (use boto3 instead)
- âŒ Hardcoded resource names (use terraform outputs or boto3 discovery)
- âŒ Breaking the unified workflow approach
- âŒ Storing actual credentials in files (use templates instead)

## Project History Context

This repository evolved from separate terraform-local.yaml and terraform.yaml files into a unified intelligent workflow. The decision was made to:

1. **Consolidate workflows** to follow DRY principle
1. **Simplify authentication** using access keys instead of OIDC
1. **Add comprehensive tooling** for credential management and cleanup
1. **Implement clean slate testing** with destroy/apply cycles

Understanding this evolution helps explain current architecture decisions and the emphasis on unified, intelligent workflows that adapt to different environments.

## Lessons Learned - September 24, 2025

### **TypedDict Safety and Error Prevention**

**Issue Discovered**: Unsafe dictionary access patterns in `create_bootstrap.py` line 56 caused potential runtime errors when accessing AWS ClientError response structures.

**Root Cause**: Direct dictionary key access `e.response["Error"]["Code"]` is unsafe when AWS API responses may vary in structure.

**Solution Implemented**: Created `get_error_code()` helper function using safe `.get()` patterns:

```python
def get_error_code(e: ClientError) -> Optional[str]:
    """Safely extract error code from AWS ClientError."""
    return e.response.get("Error", {}).get("Code")
```

**Key Lessons**:

- Always use `.get()` for nested dictionary access in AWS responses
- TypedDict patterns require defensive programming
- Helper functions improve code safety and readability
- AWS API responses can have varying structures depending on error types

### **Pylance Integration and MCP Framework**

**Achievement**: Successfully implemented comprehensive Pylance error detection system integrated into validation pipeline.

**Components Created**:

1. **`scripts/collect_pylance_errors.py`**: Basic error collection framework
1. **`scripts/pylance_check.py`**: MCP-integrated error checking
1. **`scripts/pylance_check_mcp.py`**: Full MCP framework with TypedDict safety monitoring
1. **Makefile Integration**: Added `pylance-check` target to `make validate` pipeline

**MCP Integration Benefits**:

- Direct access to VS Code's Python Language Server capabilities
- Real-time error detection using `mcp_pylance_mcp_s_pylanceFileSyntaxErrors`
- Workspace-wide file discovery with `mcp_pylance_mcp_s_pylanceWorkspaceUserFiles`
- TypedDict safety pattern detection and monitoring

**Key Technical Insights**:

- MCP tools provide powerful automation capabilities for development workflows
- Pylance error detection can be fully automated and integrated into CI/CD
- JSON output format enables easy integration with other tools
- TypedDict safety can be systematically monitored across entire codebase

### **S3 Bucket Propagation and Timing Issues**

**Issue Discovered**: Terraform initialization failing with "S3 bucket does not exist" despite successful bucket creation.

**Root Cause**: AWS S3 bucket creation involves eventual consistency - bucket exists but may not be immediately available for all operations.

**Solution Implemented**: Added comprehensive propagation detection with exponential backoff:

```python
def wait_for_s3_bucket_availability(bucket_name: str, region: str = "us-east-1", max_attempts: int = 6):
    """Wait for S3 bucket to be available for Terraform backend operations."""
    # Tests both head_bucket() and list_objects_v2() (what Terraform uses)
    # Exponential backoff: 1s, 2s, 4s, 8s, 16s, 32s delays
```

**Key Lessons**:

- AWS resource creation involves eventual consistency
- Different AWS operations may have different availability timing
- Test the actual operations that will be performed (not just resource existence)
- Exponential backoff patterns from other AWS operations can be reused
- Clear progress messaging improves user experience during waits

### **Development Workflow Enhancement**

**Full Test Pipeline Success**: The `make full-test YES=1` command now successfully completes all phases:

1. Complete cleanup using root credentials
1. Fresh bootstrap setup with S3 backend
1. Infrastructure deployment with proper propagation waits
1. Credential generation and GitHub secrets configuration
1. Comprehensive testing (local, Act, GitHub Actions)

**Process Improvements**:

- Added automatic confirmation flags for destructive operations
- Integrated propagation waits into existing flows
- Enhanced error reporting and progress tracking
- Validated end-to-end pipeline reliability

### **Code Quality and Automation Standards**

**Validation Pipeline Enhancement**:

- Successfully integrated Pylance checking into `make validate`
- All code quality checks now include TypedDict safety monitoring
- Zero-error policy maintained across entire codebase
- Automated error detection prevents runtime issues

**Best Practices Reinforced**:

- Always include comprehensive error handling for AWS operations
- Use consistent patterns for retry logic and propagation waits
- Integrate quality checks into primary development workflows
- Maintain clear documentation of complex error handling patterns

### **Future Considerations**

**Scalability Patterns**: The MCP integration framework can be extended to:

- Monitor other Python static analysis tools
- Integrate additional VS Code Language Server capabilities
- Provide real-time code quality feedback during development

**Infrastructure Reliability**: The propagation wait patterns should be applied to:

- IAM resource creation and availability
- Other AWS services with eventual consistency characteristics
- Cross-service dependency timing issues

**Development Efficiency**: The comprehensive error detection system provides:

- Early error detection before deployment
- Consistent code quality across all contributors
- Automated enforcement of safety patterns
