# AI Context and Guidance for cvideo-click-pave Repository

This file provides comprehensive guidance for AI assistants working on this repository. Read this file first before making any changes to understand the project's architecture, patterns, and constraints.

## Project Overview

**Repository Purpose**: AWS infrastructure provisioning for the cvideo-click project using Infrastructure as Code (IaC) principles.

**Core Technologies**:
- **Terraform**: Primary infrastructure management tool
- **Python + boto3**: All automation scripts and AWS interactions  
- **Make**: Workflow orchestration and task automation
- **GitHub Actions**: CI/CD pipeline with intelligent local/production detection

**No Bash/Shell Scripts**: All automation must use Python and boto3. Bash scripts are deprecated.

## Code Quality Standards

### **Python Development Standards**
- **Formatter**: Black with 88-character line length
- **Linting**: Flake8 with E203/W503 exceptions for Black compatibility
- **Type Checking**: mypy with relaxed settings for boto3 compatibility
- **Documentation**: PEP257 compliant docstrings for all functions
- **Logging**: Structured logging with timestamps and proper levels
- **Error Handling**: Comprehensive exception handling with boto3 ClientError patterns

### **Configuration Files**
- `pyproject.toml`: Black and mypy configurations
- `requirements.txt`: Production and development dependencies including boto3-stubs
- `Makefile`: Code quality targets (format, lint, type-check, validate)

### **Development Workflow**
```bash
make format      # Format code with Black
make lint        # Lint with Flake8
make type-check  # Type checking with mypy
make validate    # Comprehensive validation (includes formatting + linting + terraform)
make state-show  # Display current Terraform state resources
make state-pull  # Pull remote state for local inspection
make state-backup # Create timestamped backup of current state
```

### **Terraform State Management**
- **Remote S3 Backend**: `pave-tf-state-bucket-us-east-1` with key `pave/terraform.tfstate`
- **Shared State**: All deployment methods (local, Act, GitHub Actions) use the same S3 backend
- **Migration**: Completed migration from local backend to S3 remote backend (23 resources)
- **State Operations**: Makefile includes state management targets for backup and inspection

## Architecture Principles

### 1. **Unified Workflow Philosophy**
- Single GitHub Actions workflow (`terraform.yaml`) that intelligently adapts to environments
- Environment detection via `${{ env.ACT }}` for local (Act) vs production (GitHub Actions)
- Consistent behavior across local development and CI/CD
- **Shared Terraform State**: S3 remote backend ensures state consistency across all deployment methods

### 2. **Technology Stack Constraints**
- ✅ **Terraform**: Infrastructure definition and state management (S3 remote backend)
- ✅ **Python + boto3**: All AWS interactions and automation scripts
- ✅ **Make**: Task orchestration (replaces complex shell commands)
- ✅ **GitHub Actions**: CI/CD pipeline
- ✅ **Black Formatter**: Consistent Python code formatting
- ✅ **Type Annotations**: Full typing support with mypy
- ❌ **No Bash/Shell scripts**: Use Python instead
- ❌ **No AWS CLI calls in scripts**: Use boto3 programmatically

### 3. **Authentication Strategy**
- **Local Development**: AWS access keys via `.secrets` file
- **GitHub Actions**: AWS access keys via repository secrets (simplified approach)
- **No OIDC**: Avoided due to chicken-and-egg problem (this repo creates the OIDC role)

### 4. **Resource Naming Convention**
All AWS resources use unique random suffixes to prevent conflicts:
```
admin-user-{8-char-random}
developer-user-{8-char-random}  
CICDDeploymentRole-{8-char-random}
pave-tf-state-bucket-{region}-{8-char-random}
```

### 5. **Security Model - Bootstrap User Pattern**

**CRITICAL**: This repository implements a bootstrap user security model:
- **Bootstrap User**: `pave-bootstrap-user` - Created manually by AWS root account
- **Bootstrap Role**: `PaveBootstrapRole` - Administrative role for infrastructure management
- **Bootstrap Policy**: `PaveBootstrapPolicy` - Full permissions except cannot delete bootstrap resources
- **Purpose**: Provides secure, permanent administrative access for infrastructure management
- **Protection**: NEVER managed by Terraform, NEVER deleted by cleanup scripts

#### **Managed Tier (Terraform Created)**
- **Admin User**: Limited administrative access, cannot delete bootstrap resources
- **Developer User**: Limited access (S3 + Lambda + EC2 read-only) for application development  
- **CI/CD Roles**: Specific permissions for deployment automation
- **Protection**: Managed by Terraform, can be cleaned up safely

#### **Authentication Flow**
1. **Repository Authentication**: Uses bootstrap user credentials in secrets
2. **All Operations**: Run as bootstrap user with full administrative privileges
3. **Created Resources**: Cannot modify or delete bootstrap user/role/policy

#### **Security Constraints**
- **Admin users cannot delete bootstrap resources**: Explicit deny policies prevent this
- **Credential Files**: Template-based approach due to AWS access key secrets being non-retrievable
- **File Permissions**: 600 for all credential files
- **Bootstrap Resources**: Protected from all automation and cleanup operations
- **Git Ignore**: All credentials and sensitive files excluded

## File Structure and Responsibilities

```
├── .github/workflows/
│   └── terraform.yaml         # Unified intelligent workflow
├── scripts/                   # Python automation scripts (PEP 8 compliant)
│   ├── __init__.py
│   ├── credentials.py         # Credential management with full typing and logging
│   └── cleanup.py             # AWS resource cleanup with comprehensive error handling
├── pave_infra.tf              # Main Terraform configuration
├── Makefile                   # Task orchestration with code quality targets
├── requirements.txt           # Production and development dependencies
├── pyproject.toml             # Black and mypy configuration
├── .ai-context.md             # This file - AI guidance
├── .secrets                   # Local AWS credentials (gitignored)
├── .actrc                     # Act configuration
├── credentials/               # Generated credential templates (gitignored)
├── GITHUB_SETUP.md            # Repository secrets setup guide
└── README.md                  # User documentation
```

## Makefile Interface Design

The Makefile provides these primary targets with integrated code quality:

```makefile
# Core Infrastructure Operations
make init          # Initialize terraform and install Python deps
make plan          # Terraform plan
make apply         # Terraform apply (deploy infrastructure)  
make destroy       # Terraform destroy
make clean         # Comprehensive cleanup of all AWS resources

# Credential Management
make credentials   # Extract/generate credential templates
make setup-github  # Set up GitHub repository secrets

# Development Workflow
make dev-deploy    # Local development deployment (clean slate)
make dev-clean     # Clean up development resources

# Code Quality
make format        # Format code with Black
make lint          # Lint code with Flake8
make type-check    # Type check with mypy
make validate      # Validate terraform + Python code (includes formatting/linting)
make test          # Run tests if any

# Help
make help          # Show all available targets
```

## Python Script Guidelines

### 1. **Code Quality Standards**
- **Black Formatting**: 88-character line length, PEP8 compliant
- **Type Annotations**: Full typing with boto3-stubs support
- **Logging**: Structured logging with proper levels and timestamps
- **Error Handling**: Comprehensive boto3 ClientError handling
- **Documentation**: PEP257 compliant docstrings

### 2. **scripts/credentials.py**
- Professional-grade credential management with full type annotations
- Comprehensive error handling and structured logging
- Proper file permissions and security practices
- Clear user guidance for manual credential extraction

### 3. **scripts/cleanup.py** 
- Comprehensive AWS resource cleanup with bootstrap protection
- Type-safe boto3 operations with proper exception handling
- Structured logging with progress reporting
- Resource validation and rollback capabilities

### 4. **Common Patterns for Python Scripts**
```python
"""Module docstring following PEP257."""
import logging
from typing import Optional, Dict, Any
import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_boto3_client(service: str) -> boto3.client:
    """Get boto3 client with proper error handling and type hints."""
    try:
        return boto3.client(service)
    except NoCredentialsError:
        logger.error(f"No AWS credentials found for {service}")
        raise
    except ClientError as e:
        logger.error(f"Error connecting to AWS {service}: {e}")
        raise

def process_resources(client: boto3.client, pattern: str) -> Optional[Dict[str, Any]]:
    """Process AWS resources with comprehensive error handling."""
    try:
        # Implementation with proper type hints and logging
        logger.info(f"Processing resources with pattern: {pattern}")
        # ... implementation
    except ClientError as e:
        logger.error(f"AWS API error: {e}")
        return None
```

## Workflow Integration Points

### 1. **Local Development with Act**
- Use `make dev-deploy` which calls terraform via Act
- Automatically runs cleanup first for clean slate testing
- Uses `.secrets` file for authentication

### 2. **GitHub Actions**
- Triggered by push to main or workflow_dispatch
- Uses repository secrets for authentication
- Calls `make plan` and `make apply`

### 3. **Direct Terraform**
- `make init`, `make plan`, `make apply` for traditional workflow
- Python scripts handle pre/post terraform operations

## Error Handling Patterns

### 1. **Graceful Degradation**
- If terraform outputs not available, fall back to AWS API queries
- If resources don't exist, continue without errors (idempotent operations)
- Clear error messages with actionable suggestions

### 2. **Validation**
- Validate AWS credentials before operations
- Check for required tools (terraform, python, boto3)
- Validate terraform configuration before apply

### 3. **Rollback Capabilities**
- Cleanup operations should be idempotent
- Provide dry-run modes for destructive operations
- State preservation during partial failures

## Testing Strategy

### 1. **Local Testing**
- Use Act for GitHub Actions workflow testing
- `make dev-deploy` for full infrastructure testing
- `make clean` for cleanup validation

### 2. **Validation Commands**
- `make validate` should check terraform syntax and Python code
- AWS connectivity tests
- Required dependency checks

## Documentation Standards

### 1. **README.md**
- Document Makefile interface as primary user interface
- Include quick start with Makefile commands
- Troubleshooting section with make targets

### 2. **Code Comments**
- Python scripts should have docstrings for all functions
- Makefile targets should have comment descriptions
- Terraform resources should have meaningful descriptions

## Migration from Shell Scripts

### Deprecated Files to Remove
- `extract-credentials.sh` → Replace with `scripts/credentials.py`
- `get-credentials.sh` → Replace with `scripts/credentials.py` 
- `cleanup-all.sh` → Replace with `scripts/cleanup.py`

### Functionality Preservation
Ensure all functionality from shell scripts is preserved in Python equivalents:

1. **Credential extraction** with terraform output fallback to boto3
2. **Template file generation** with AWS Console instructions
3. **Comprehensive cleanup** of all pave resources across deployments
4. **Progress reporting** with clear status messages
5. **Error handling** for missing resources or permissions

## AI Assistant Guidelines

### When Working on This Repository

1. **Read this file first** to understand architecture and constraints
2. **Use Python + boto3** instead of AWS CLI or shell scripts
3. **Maintain Makefile interface** as primary user interaction
4. **Preserve security model** with proper credential handling
5. **Test with Act** before suggesting GitHub Actions changes
6. **Update documentation** when changing interfaces

### Common Tasks

- **Adding new infrastructure**: Update `pave_infra.tf` and relevant Makefile targets
- **AWS interactions**: Always use boto3 in Python scripts, not AWS CLI
- **Workflow changes**: Test locally with Act first
- **Credential management**: Use template-based approach due to AWS limitations

### Anti-Patterns to Avoid

- ❌ Using bash/shell scripts for new functionality
- ❌ AWS CLI calls in automation (use boto3 instead)
- ❌ Hardcoded resource names (use terraform outputs or boto3 discovery)
- ❌ Breaking the unified workflow approach
- ❌ Storing actual credentials in files (use templates instead)

## Project History Context

This repository evolved from separate terraform-local.yaml and terraform.yaml files into a unified intelligent workflow. The decision was made to:

1. **Consolidate workflows** to follow DRY principle
2. **Simplify authentication** using access keys instead of OIDC
3. **Add comprehensive tooling** for credential management and cleanup
4. **Implement clean slate testing** with destroy/apply cycles

Understanding this evolution helps explain current architecture decisions and the emphasis on unified, intelligent workflows that adapt to different environments.